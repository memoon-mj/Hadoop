{"paragraphs":[{"title":"총 상권 수는?","text":"%spark2.sql\nselect count(distinct business_area_name), count(distinct id_)\nfrom Store","user":"anonymous","dateUpdated":"2019-12-15T06:59:05+0000","config":{"colWidth":5,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql"},"title":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"count(DISTINCT business_area_name)\tcount(DISTINCT id_)\n291\t746\n"}]},"apps":[],"jobName":"paragraph_1576369580005_-514960506","id":"20191215-002620_382604184","dateCreated":"2019-12-15T00:26:20+0000","dateStarted":"2019-12-15T01:27:02+0000","dateFinished":"2019-12-15T01:36:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:939"},{"title":"상권별 가게 수는?","text":"%spark2.sql\nselect business_area_name, count(distinct(id_)) as storecount \nfrom Store \ngroup by business_area_name \norder by storecount desc limit 10","user":"anonymous","dateUpdated":"2019-12-15T07:00:32+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql"},"title":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"business_area_name\tstorecount\n홍익대학교 주변\t43\n잠실 관광특구\t38\n신사동 가로수길\t36\n명동 남대문 북창동 다동 무교동 관광특구\t34\n영등포전통시장\t33\n이태원 관광특구\t26\n코엑스\t24\n압구정 로데오거리_1\t14\n강남구 신사역_2\t13\n압구정 로데오거리_2\t13\n"}]},"apps":[],"jobName":"paragraph_1576369711490_-61959453","id":"20191215-002831_587118715","dateCreated":"2019-12-15T00:28:31+0000","dateStarted":"2019-12-15T02:22:33+0000","dateFinished":"2019-12-15T02:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:940"},{"title":"어느 시간대에 사람들이 많이 몰릴까? - 요일 별 peek 시간대","text":"%spark2.sql\nselect dayofweek, first(hour), max(avg_popular) \nfrom (select dayofweek, hour, round(avg(popular_data),2) as avg_popular from Populartime \n    group by dayofweek, hour)\nwhere not dayofweek is null \ngroup by dayofweek","user":"anonymous","dateUpdated":"2019-12-15T07:04:28+0000","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"stackedAreaChart","height":300,"optionOpen":true,"setting":{"stackedAreaChart":{}},"commonSetting":{},"keys":[{"name":"dayofweek","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"max(avg_popular)","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"title":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dayofweek\tfirst(hour, false)\tmax(avg_popular)\nWednesday\t06:00:00\t37.74\nTuesday\t07:00:00\t38.06\nThurday\t05:00:00\t38.03\nFriday\t14:00:00\t40.1\nSaturday\t20:00:00\t59.58\nMonday\t21:00:00\t32.88\nSunday\t12:00:00\t56.61\n"}]},"apps":[],"jobName":"paragraph_1576369698816_-920749646","id":"20191215-002818_916576215","dateCreated":"2019-12-15T00:28:18+0000","dateStarted":"2019-12-15T07:03:12+0000","dateFinished":"2019-12-15T07:30:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:941"},{"title":"골목상권 중 사람들에게 인기가 많은 곳 top 10","text":"%spark2.sql\nselect first(business_distinct_code), first(week), first(business_area_name), count(distinct id_) as storecount\nfrom Store \nwhere week=2 and business_distinct_code='A' \ngroup by business_area_name \norder by storecount desc limit 10","user":"anonymous","dateUpdated":"2019-12-15T07:16:30+0000","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"setting":{"pieChart":{}},"commonSetting":{},"keys":[{"name":"first(business_area_name, false)","index":2,"aggr":"sum"}],"groups":[],"values":[{"name":"storecount","index":3,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"first(business_distinct_code, false)\tfirst(week, false)\tfirst(business_area_name, false)\tstorecount\nA\t2\t백제고분로32길\t4\nA\t2\t디지털로32가길\t4\nA\t2\t사가정로42길\t4\nA\t2\t이태원로27길\t4\nA\t2\t동교로17길\t3\nA\t2\t화곡로64길\t3\nA\t2\t동작대로33길\t3\nA\t2\t매봉산로2길\t3\nA\t2\t양화로1길\t3\nA\t2\t이태원로54길\t3\n"}]},"apps":[],"jobName":"paragraph_1576371898944_1268565479","id":"20191215-010458_1224042978","dateCreated":"2019-12-15T01:04:58+0000","dateStarted":"2019-12-15T03:27:19+0000","dateFinished":"2019-12-15T03:33:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:942"},{"title":"리뷰가 많고 평점이 높은 상권 Top 10","text":"%spark2.sql\nselect business_area_name, round(avg(rating),2), round(avg(rating_n),2) \nfrom Store join Store_rating on Store.id_ = Store_rating.id_ \ngroup by business_area_name \norder by avg(rating_n) desc, avg(rating) desc limit 10","user":"anonymous","dateUpdated":"2019-12-15T07:13:47+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"business_area_name\tround(avg(CAST(rating AS DOUBLE)), 2)\tround(avg(CAST(rating_n AS BIGINT)), 2)\n성안로3길\t4.2\t14685.0\n삼성역_3\t4.2\t14553.33\n서울 동작구 노량진역\t3.95\t7966.5\n서울시립대로29길\t4.1\t6007.0\n종각역\t4.2\t5091.33\n서울 종로구 종로3가역_2\t4.1\t4432.67\n국사봉2길\t4.4\t4420.0\n여의대방로22카길\t4.4\t4405.0\n망원월드컵시장\t4.2\t3829.2\n성미산로1길\t4.2\t3824.5\n"}]},"apps":[],"jobName":"paragraph_1576369150277_449547503","id":"20191215-001910_860333108","dateCreated":"2019-12-15T00:19:10+0000","dateStarted":"2019-12-15T03:50:28+0000","dateFinished":"2019-12-15T04:03:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:943"},{"title":"상권별로 평점이 높은 장소 top 5","text":"%spark2.sql\nselect * from (select distinct s.business_area_name,r.week, s.store_name, r.rating_n, r.rating, row_number() over (partition by s.business_area_name order by rating_n desc, rating desc) as ranking \nfrom (select distinct id_,business_area_name, store_name \nfrom Store) as s join Store_rating as r on s.id_ = r.id_ \nwhere r.week = 2 \norder by s.business_area_name desc, r.rating_n desc, r.rating desc) as a \nwhere a.ranking <=5\nlimit 10","user":"anonymous","dateUpdated":"2019-12-15T07:46:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: cannot resolve '`s.business_area_name`' given input columns: [a.business_area_name, a.store_name, a.rating_n, a.week, a.ranking, a.rating]; line 7 pos 9;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['s.business_area_name DESC NULLS LAST], true\n      +- Project [business_area_name#63, week#32, store_name#64, rating_n#36, rating#35, ranking#608]\n         +- Filter (ranking#608 <= 5)\n            +- SubqueryAlias a\n               +- Sort [business_area_name#63 DESC NULLS LAST, rating_n#36 DESC NULLS LAST, rating#35 DESC NULLS LAST], true\n                  +- Distinct\n                     +- Project [business_area_name#63, week#32, store_name#64, rating_n#36, rating#35, ranking#608]\n                        +- Project [business_area_name#63, week#32, store_name#64, rating_n#36, rating#35, ranking#608, ranking#608]\n                           +- Window [row_number() windowspecdefinition(business_area_name#63, rating_n#36 DESC NULLS LAST, rating#35 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS ranking#608], [business_area_name#63], [rating_n#36 DESC NULLS LAST, rating#35 DESC NULLS LAST]\n                              +- Project [business_area_name#63, week#32, store_name#64, rating_n#36, rating#35]\n                                 +- Filter (week#32 = 2)\n                                    +- Join Inner, (id_#58 = id_#30)\n                                       :- SubqueryAlias s\n                                       :  +- Distinct\n                                       :     +- Project [id_#58, business_area_name#63, store_name#64]\n                                       :        +- SubqueryAlias store\n                                       :           +- Relation[id_#58,date#59,week#60,business_distinct_code#61,business_area_code#62,business_area_name#63,store_name#64,address#65,lng#66,lat#67] csv\n                                       +- SubqueryAlias r\n                                          +- SubqueryAlias store_rating\n                                             +- Relation[id_#30,date#31,week#32,business_distinct_code#33,business_area_code#34,rating#35,rating_n#36,timespent_min#37,timespent_max#38] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:119)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:101)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:502)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1576367100878_388846954","id":"20191214-234500_604523124","dateCreated":"2019-12-14T23:45:00+0000","dateStarted":"2019-12-15T07:19:13+0000","dateFinished":"2019-12-15T07:31:37+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:944"},{"text":"%spark2.sql\nselect business_area_name,store_name, timespent_min/60 as timespent_min_hour, timespent_max/60 as timespent_max_hour, ((timespent_max-timespent_min)/60) as time_spent \nfrom Store join Store_rating on Store.id_ = Store_rating.id_ \ngroup by store_name \norder by time_spent desc limit 5","user":"anonymous","dateUpdated":"2019-12-15T07:30:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: expression 'store.`business_area_name`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nGlobalLimit 5\n+- LocalLimit 5\n   +- Sort [time_spent#612 DESC NULLS LAST], true\n      +- Aggregate [store_name#64], [business_area_name#63, store_name#64, (cast(timespent_min#37 as double) / cast(60 as double)) AS timespent_min_hour#610, (cast(timespent_max#38 as double) / cast(60 as double)) AS timespent_max_hour#611, (cast((timespent_max#38 - timespent_min#37) as double) / cast(60 as double)) AS time_spent#612]\n         +- Join Inner, (id_#58 = id_#30)\n            :- SubqueryAlias store\n            :  +- Relation[id_#58,date#59,week#60,business_distinct_code#61,business_area_code#62,business_area_name#63,store_name#64,address#65,lng#66,lat#67] csv\n            +- SubqueryAlias store_rating\n               +- Relation[id_#30,date#31,week#32,business_distinct_code#33,business_area_code#34,rating#35,rating_n#36,timespent_min#37,timespent_max#38] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:220)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:119)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:101)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:502)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1576370538918_-1760152148","id":"20191215-004218_604203164","dateCreated":"2019-12-15T00:42:18+0000","dateStarted":"2019-12-15T07:30:40+0000","dateFinished":"2019-12-15T07:31:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:945"},{"text":"%spark2.sql\n","user":"anonymous","dateUpdated":"2019-12-15T07:20:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576394440618_294992595","id":"20191215-072040_39404893","dateCreated":"2019-12-15T07:20:40+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:946"}],"name":"bigdata project","id":"2EXTAQKZN","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}